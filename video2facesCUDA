import face_recognition
import cv2
import time
from PIL import Image
from datetime import datetime
import argparse
import os
import csv
import numpy as np
from sklearn.cluster import DBSCAN

# === UTILITY ===
def parse_time_string(time_str):
    h, m, s = map(int, time_str.split(":"))
    return h * 3600 + m * 60 + s

def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def save_face_by_cluster(face_encoding, face_image, timestamp, frame_number, base_dir, cluster_id):
    person_dir = os.path.join(base_dir, f"person_{cluster_id}")
    ensure_dir(person_dir)
    filename = f"{int(timestamp)}s_{frame_number}.jpg"
    save_path = os.path.join(person_dir, filename)
    Image.fromarray(face_image).save(save_path)
    return save_path

# === ARGOMENTI ===
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--input", required=True, help="Video file di input")
ap.add_argument("-o", "--output", required=True, help="Directory di output")
ap.add_argument("--start", type=str, default="00:00:00", help="Inizio (hh:mm:ss)")
ap.add_argument("--finish", type=str, help="Fine (hh:mm:ss)")
ap.add_argument("--use-cuda", action="store_true", help="Usa CUDA per CNN")
args = vars(ap.parse_args())

# === PERCORSI E LOG ===
faces_root = os.path.join(args["output"], "faces")
frames_root = os.path.join(args["output"], "frames")
annotated_root = os.path.join(args["output"], "annotated_frames")
log_csv = os.path.join(args["output"], "log.csv")
ensure_dir(faces_root)
ensure_dir(frames_root)
ensure_dir(annotated_root)

# === VIDEO ===
video = cv2.VideoCapture(args["input"])
fps = video.get(cv2.CAP_PROP_FPS)
total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
duration = total_frames / fps
video_name = os.path.basename(args["input"])

start_sec = parse_time_string(args["start"])
end_sec = parse_time_string(args["finish"]) if args["finish"] else duration
start_frame = int(start_sec * fps)
end_frame = int(end_sec * fps)

model_type = "cnn" if args["use_cuda"] else "hog"
print(f"üìº Video: {video_name} | FPS: {fps:.2f}")
print(f"‚è© Analisi da {args['start']} a {args['finish'] or 'fine'} con modello: {model_type.upper()}")

# === LOG CSV ===
csv_file = open(log_csv, mode='w', newline='')
csv_writer = csv.writer(csv_file)
csv_writer.writerow(["video", "timestamp", "top", "right", "bottom", "left", "cluster", "face_path", "frame_path", "annotated_frame_path"])

# === ANALISI ===
frame_number = 0
face_encodings = []
face_images = []
face_info = []

video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
start_time = time.time()

while frame_number <= end_frame:
    ret, frame = video.read()
    if not ret:
        break

    if frame_number < start_frame:
        frame_number += 1
        continue

    timestamp_sec = frame_number / fps
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    annotated_frame = frame.copy()

    locations = face_recognition.face_locations(rgb_frame, model=model_type)
    encodings = face_recognition.face_encodings(rgb_frame, locations)

    print(f"[Frame {frame_number}/{end_frame}] ‚è± {timestamp_sec:.2f}s - Volti: {len(locations)}")

    for i, (top, right, bottom, left) in enumerate(locations):
        face_img = rgb_frame[top:bottom, left:right]
        face_images.append(face_img)
        face_encodings.append(encodings[i])
        face_info.append((timestamp_sec, frame_number, (top, right, bottom, left), frame.copy()))

    # Mostra anteprima live
    for (top, right, bottom, left) in locations:
        cv2.rectangle(annotated_frame, (left, top), (right, bottom), (0, 0, 255), 2)

    cv2.imshow("Anteprima", annotated_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

    frame_number += 1

# === CLUSTERING ===
print("\nü§ñ Clustering volti per persona...")
if face_encodings:
    X = np.array(face_encodings)
    clustering = DBSCAN(metric="euclidean", eps=0.6, min_samples=1).fit(X)
else:
    clustering = None

# === SALVATAGGIO ===
for idx, (encoding, face_img, info) in enumerate(zip(face_encodings, face_images, face_info)):
    timestamp, frame_num, (top, right, bottom, left), full_frame = info
    cluster_id = clustering.labels_[idx] if clustering else 0

    # Salva volto
    face_path = save_face_by_cluster(encoding, face_img, timestamp, frame_num, faces_root, cluster_id)

    # Salva frame originale
    frame_path = save_face_by_cluster(encoding, cv2.cvtColor(full_frame, cv2.COLOR_BGR2RGB), timestamp, frame_num, frames_root, cluster_id)

    # Salva frame annotato
    annotated = full_frame.copy()
    cv2.rectangle(annotated, (left, top), (right, bottom), (0, 0, 255), 2)
    annotated_path = save_face_by_cluster(encoding, annotated, timestamp, frame_num, annotated_root, cluster_id)

    csv_writer.writerow([
        video_name,
        round(timestamp, 2),
        top, right, bottom, left,
        f"person_{cluster_id}",
        face_path,
        frame_path,
        annotated_path
    ])

# === FINE ===
video.release()
csv_file.close()
cv2.destroyAllWindows()
elapsed = time.time() - start_time
print(f"\n‚úÖ Completato in {elapsed:.1f}s")
print(f"üìù Log CSV: {log_csv}")
